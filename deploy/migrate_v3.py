import os
import json
import sqlite3
import logging
from typing import Dict, Any, Optional, List, Tuple

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =============================================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RESULTS_DIR = os.path.join(BASE_DIR, 'results_data')
DATABASE_PATH = os.path.join(BASE_DIR, 'app_data.db')
TARGET_DB_VERSION = 3  # –ò–ó–ú–ï–ù–ï–ù–û: –¶–µ–ª–µ–≤–∞—è –≤–µ—Ä—Å–∏—è —Å—Ö–µ–º—ã —Ç–µ–ø–µ—Ä—å 3
BATCH_SIZE = 100

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s'
)

# =============================================================================
# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¶–ï–õ–ï–í–û–ô –°–•–ï–ú–´ –ë–ê–ó–´ –î–ê–ù–ù–´–• (V3)
# =============================================================================

def get_target_schema() -> Dict[str, str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å CREATE-–∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ö–µ–º—ã V3.
    –°—Ö–µ–º–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–µ–π app.py.
    """
    return {
        'users': '''
            CREATE TABLE users (
                user_id INTEGER PRIMARY KEY AUTOINCREMENT,
                lastname TEXT, firstname TEXT, middlename TEXT, position TEXT,
                persistent_id TEXT UNIQUE NOT NULL
            )
        ''',
        'fingerprints': '''
            CREATE TABLE fingerprints (
                fingerprint_hash TEXT PRIMARY KEY,
                user_agent TEXT, platform TEXT, webgl_renderer TEXT,
                first_seen TEXT NOT NULL, last_seen TEXT NOT NULL
            )
        ''',
        'result_metadata': '''
            CREATE TABLE result_metadata (
                session_id TEXT PRIMARY KEY, user_id INTEGER, fingerprint_hash TEXT,
                test_type TEXT, score INTEGER, start_time TEXT, end_time TEXT, -- –ò–ó–ú–ï–ù–ï–ù–û: score REAL -> INTEGER
                filename TEXT NOT NULL, created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP, -- –î–û–ë–ê–í–õ–ï–ù–û: created_at
                FOREIGN KEY(user_id) REFERENCES users(user_id) ON DELETE SET NULL,
                FOREIGN KEY(fingerprint_hash) REFERENCES fingerprints(fingerprint_hash) ON DELETE SET NULL
            )
        ''',
        'document_counters': '''
            CREATE TABLE document_counters (
                period TEXT PRIMARY KEY,
                last_sequence_number INTEGER NOT NULL,
                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP -- –î–û–ë–ê–í–õ–ï–ù–û: updated_at
            )
        ''',
        'proctoring_events': '''
            CREATE TABLE proctoring_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT NOT NULL, event_type TEXT NOT NULL, event_timestamp TEXT NOT NULL,
                details TEXT, persistent_id TEXT, client_ip TEXT, page TEXT,
                created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP -- –î–û–ë–ê–í–õ–ï–ù–û: created_at
            )
        ''',
        'certificates': '''
            CREATE TABLE certificates (
                document_number TEXT PRIMARY KEY, user_fullname TEXT NOT NULL,
                user_position TEXT, test_type TEXT NOT NULL, issue_date TEXT NOT NULL,
                score_percentage INTEGER NOT NULL, session_id TEXT NOT NULL,
                created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP -- –î–û–ë–ê–í–õ–ï–ù–û: created_at
            )
        '''
    }

def get_target_indexes() -> List[str]:
    """–î–û–ë–ê–í–õ–ï–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ CREATE-–∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤."""
    return [
        'CREATE INDEX IF NOT EXISTS idx_events_session ON proctoring_events(session_id)',
        'CREATE UNIQUE INDEX IF NOT EXISTS idx_users_persistent_id ON users(persistent_id)',
        'CREATE INDEX IF NOT EXISTS idx_metadata_user_id ON result_metadata(user_id)',
    ]

# =============================================================================
# –ö–õ–ê–°–° –ú–ò–ì–†–ê–¶–ò–ò (–æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∫–ª–∞—Å—Å–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –∫—Ä–æ–º–µ run)
# =============================================================================

class DatabaseMigrator:
    def __init__(self, db_path: str, results_dir: str):
        self.db_path = db_path
        self.results_dir = results_dir
        self.conn: Optional[sqlite3.Connection] = None
        self.user_cache: Dict[str, int] = {}

    def _connect(self):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row
            self.conn.execute("PRAGMA journal_mode=WAL;")
            self.conn.execute("PRAGMA foreign_keys = ON;")
            logging.info(f"–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {self.db_path}")
        except sqlite3.Error as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise

    def _close(self):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        if self.conn:
            self.conn.close()
            logging.info("–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ.")

    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –º–∏–≥—Ä–∞—Ü–∏–∏."""
        self._connect()
        if not self.conn: return

        try:
            cursor = self.conn.cursor()
            current_version = cursor.execute("PRAGMA user_version").fetchone()[0]
            if current_version >= TARGET_DB_VERSION:
                logging.info(f"–ú–∏–≥—Ä–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è. –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è —Å—Ö–µ–º—ã ({current_version}) —É–∂–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç V{TARGET_DB_VERSION}.")
                return

            logging.info(f"–ù–∞—á–∞–ª–æ –º–∏–≥—Ä–∞—Ü–∏–∏ —Å–æ —Å—Ö–µ–º—ã v{current_version} –Ω–∞ v{TARGET_DB_VERSION}...")
            
            cursor.execute("BEGIN;")
            self._backup_and_recreate_schema(cursor)
            self._migrate_from_json_files(cursor)
            self._migrate_from_old_tables(cursor)
            
            # –î–û–ë–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
            logging.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤...")
            for index_sql in get_target_indexes():
                cursor.execute(index_sql)
            logging.info("–ò–Ω–¥–µ–∫—Å—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã.")

            cursor.execute(f"PRAGMA user_version = {TARGET_DB_VERSION};")
            
            self.conn.commit()
            logging.info("–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞. –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ V%s –∑–∞–≤–µ—Ä—à–µ–Ω–∞.", TARGET_DB_VERSION)
            self._print_report()

        except Exception as e:
            # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            logging.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –º–∏–≥—Ä–∞—Ü–∏–∏: {e}", exc_info=True)
            if self.conn:
                self.conn.rollback()
                logging.warning("–í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–º–µ–Ω–µ–Ω—ã –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏.")
        finally:
            self._close()

    def _backup_and_recreate_schema(self, cursor: sqlite3.Cursor):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        logging.info("–ù–∞—á–∞–ª–æ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è —Å—Ö–µ–º—ã...")
        schema = get_target_schema()
        
        for table_name in schema.keys():
            try:
                cursor.execute(f"ALTER TABLE {table_name} RENAME TO {table_name}_old")
                logging.info(f"–¢–∞–±–ª–∏—Ü–∞ '{table_name}' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –≤ '{table_name}_old'.")
            except sqlite3.OperationalError as e:
                if "no such table" in str(e):
                    logging.warning(f"–¢–∞–±–ª–∏—Ü–∞ '{table_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è.")
                else: raise
        
        logging.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü...")
        for create_sql in schema.values():
            cursor.execute(create_sql)
        logging.info("–ù–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã.")

    def _get_or_create_user_id(self, cursor: sqlite3.Cursor, user_info: Dict[str, Any], persistent_id: Optional[str]) -> Optional[int]:
        # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NOT NULL –¥–ª—è persistent_id
        if not persistent_id:
            # –ï—Å–ª–∏ –Ω–µ—Ç persistent_id, –º—ã –Ω–µ –º–æ–∂–µ–º –Ω–∞–¥–µ–∂–Ω–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            # –ú–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å "–∞–Ω–æ–Ω–∏–º–Ω–æ–≥–æ" –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–æ –ª—É—á—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –¥–ª—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏.
            return None

        if persistent_id in self.user_cache:
            return self.user_cache[persistent_id]

        cursor.execute("SELECT user_id FROM users WHERE persistent_id = ?", (persistent_id,))
        row = cursor.fetchone()
        if row:
            self.user_cache[persistent_id] = row['user_id']
            return row['user_id']

        try:
            insert_cursor = cursor.execute(
                "INSERT INTO users (lastname, firstname, middlename, position, persistent_id) VALUES (?, ?, ?, ?, ?)",
                (user_info.get('lastName'), user_info.get('firstName'), user_info.get('middleName'), user_info.get('position'), persistent_id)
            )
            user_id = insert_cursor.lastrowid
            if user_id: self.user_cache[persistent_id] = user_id
            return user_id
        except sqlite3.IntegrityError:
            cursor.execute("SELECT user_id FROM users WHERE persistent_id = ?", (persistent_id,))
            row = cursor.fetchone()
            return row['user_id'] if row else None

    def _migrate_from_json_files(self, cursor: sqlite3.Cursor):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        json_files = [f for f in os.listdir(self.results_dir) if f.endswith('.json')]
        logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(json_files)} JSON-—Ñ–∞–π–ª–æ–≤ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –≤ —Ç–∞–±–ª–∏—Ü—ã users, fingerprints, result_metadata.")
        
        success_count = 0
        metadata_batch: List[Tuple] = []
        fingerprints_to_update: Dict[str, Tuple] = {}

        for filename in json_files:
            filepath = os.path.join(self.results_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f: data = json.load(f)
            except (json.JSONDecodeError, OSError) as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª {filename}: {e}")
                continue

            session_id = data.get('sessionId')
            if not session_id:
                logging.warning(f"–ü—Ä–æ–ø—É—Å–∫ —Ñ–∞–π–ª–∞ {filename}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç 'sessionId'.")
                continue

            user_info = data.get('userInfo', {})
            persistent_id = data.get('persistentId', {}).get('cookie')
            user_id = self._get_or_create_user_id(cursor, user_info, persistent_id)
            
            fp_hash = data.get('fingerprint', {}).get('privacySafeHash')
            start_time = data.get('sessionMetrics', {}).get('startTime')
            
            if fp_hash and start_time:
                fp_data = data.get('fingerprint', {}).get('privacySafe', {})
                if fp_hash not in fingerprints_to_update or start_time > fingerprints_to_update[fp_hash][-1]:
                    fingerprints_to_update[fp_hash] = (fp_hash, fp_data.get('userAgent'), fp_data.get('platform'), fp_data.get('webGLRenderer'), start_time, start_time)
            
            metadata_batch.append((
                session_id, user_id, fp_hash,
                data.get('testType'), data.get('testResults', {}).get('percentage'),
                start_time, data.get('sessionMetrics', {}).get('endTime'),
                filename
            ))
            success_count += 1
            
            if len(metadata_batch) >= BATCH_SIZE:
                self._execute_batches(cursor, metadata_batch, list(fingerprints_to_update.values()))
                metadata_batch.clear(); fingerprints_to_update.clear()

        if metadata_batch: self._execute_batches(cursor, metadata_batch, list(fingerprints_to_update.values()))
        logging.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON-—Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ: {success_count}, –û—à–∏–±–∫–∏: {len(json_files) - success_count}")

    def _execute_batches(self, cursor: sqlite3.Cursor, metadata: List[Tuple], fingerprints: List[Tuple]):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        if fingerprints:
            cursor.executemany("INSERT OR REPLACE INTO fingerprints (fingerprint_hash, user_agent, platform, webgl_renderer, first_seen, last_seen) VALUES (?, ?, ?, ?, ?, ?)", fingerprints)
        if metadata:
            cursor.executemany("INSERT OR REPLACE INTO result_metadata (session_id, user_id, fingerprint_hash, test_type, score, start_time, end_time, filename) VALUES (?, ?, ?, ?, ?, ?, ?, ?)", metadata)

    def _migrate_from_old_tables(self, cursor: sqlite3.Cursor):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        logging.info("–ù–∞—á–∞–ª–æ –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—Ç–∞—Ä—ã—Ö —Ç–∞–±–ª–∏—Ü...")
        tables_to_migrate = ['proctoring_events', 'certificates', 'document_counters']
        
        for table in tables_to_migrate:
            try:
                cursor.execute(f"PRAGMA table_info({table})"); new_cols = {row['name'] for row in cursor.fetchall()}
                cursor.execute(f"PRAGMA table_info({table}_old)"); old_cols = {row['name'] for row in cursor.fetchall()}
                
                common_cols = sorted(list(new_cols.intersection(old_cols)))
                if not common_cols:
                    logging.warning(f"–ù–µ—Ç –æ–±—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –º–µ–∂–¥—É {table} –∏ {table}_old. –ü—Ä–æ–ø—É—Å–∫."); continue

                cols_str = ", ".join(common_cols)
                cursor.execute(f"INSERT OR IGNORE INTO {table} ({cols_str}) SELECT {cols_str} FROM {table}_old")
                logging.info(f"–î–∞–Ω–Ω—ã–µ –∏–∑ '{table}_old' —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ '{table}'.")

            except sqlite3.OperationalError as e:
                if "no such table" in str(e):
                    logging.warning(f"–°—Ç–∞—Ä–∞—è —Ç–∞–±–ª–∏—Ü–∞ '{table}_old' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ú–∏–≥—Ä–∞—Ü–∏—è –¥–ª—è –Ω–µ–µ –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
                else: raise

    def cleanup_old_tables(self):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        self._connect()
        if not self.conn: return
        logging.warning("–í–ù–ò–ú–ê–ù–ò–ï: –°–µ–π—á–∞—Å –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ —Ç–∞–±–ª–∏—Ü (_old). –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ–æ–±—Ä–∞—Ç–∏–º–æ.")
        confirm = input("–î–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤–≤–µ–¥–∏—Ç–µ 'DELETE': ")
        
        if confirm != 'DELETE':
            logging.info("–£–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ."); self._close(); return

        try:
            cursor = self.conn.cursor()
            schema = get_target_schema()
            for table_name in schema.keys():
                cursor.execute(f"DROP TABLE IF EXISTS {table_name}_old")
                logging.info(f"–¢–∞–±–ª–∏—Ü–∞ '{table_name}_old' —É–¥–∞–ª–µ–Ω–∞.")
            self.conn.commit()
            logging.info("–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ç–∞–±–ª–∏—Ü –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ: {e}"); self.conn.rollback()
        finally:
            self._close()

    def _print_report(self):
        # ... –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        print("\n" + "="*60 + "\n" + " " * 15 + "–û—Ç—á–µ—Ç –æ –º–∏–≥—Ä–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö" + "\n" + "="*60)
        print(f"–ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ —Å—Ö–µ–º—É V{TARGET_DB_VERSION} —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
        print("‚úÖ –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞.")
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑ JSON-—Ñ–∞–π–ª–æ–≤ –∏ —Å—Ç–∞—Ä—ã—Ö —Ç–∞–±–ª–∏—Ü –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã.")
        print("‚úÖ –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ–∑–¥–∞–Ω—ã.")
        print("\n–í–ê–ñ–ù–û:")
        print("üîµ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.")
        print("üîµ –°—Ç–∞—Ä—ã–µ —Ç–∞–±–ª–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º '_old'.")
        print("üîµ –î–ª—è –∏—Ö —É–¥–∞–ª–µ–Ω–∏—è —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ cleanup_old_tables().")
        print("="*60)

# =============================================================================
# –¢–û–ß–ö–ê –í–•–û–î–ê
# =============================================================================
if __name__ == '__main__':
    migrator = DatabaseMigrator(db_path=DATABASE_PATH, results_dir=RESULTS_DIR)
    migrator.run()
    
    # –ü–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ—á–∏—Å—Ç–∫—É:
    migrator.cleanup_old_tables()
