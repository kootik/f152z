# app/api/routes.py

import json
from datetime import UTC, datetime, timedelta

from flask import current_app, jsonify, request
from pydantic import ValidationError
from sqlalchemy import and_, case, func, or_, distinct, cast, Text
from sqlalchemy.exc import IntegrityError
from sqlalchemy.exc import DBAPIError

from app.auth.decorators import admin_required, api_key_required
from app.extensions import cache, csrf, db, limiter, socketio
from app.metrics import (
    ANALYSIS_DURATION_SECONDS,
    DOCUMENTS_GENERATED_TOTAL,
    TESTS_COMPLETED_TOTAL,
)
from app.models import (
    Certificate,
    DocumentCounter,
    Fingerprint,
    ProctoringEvent,
    ResultMetadata,
    User,
)
from app.schemas.result_schema import SaveResultsRequest
from app.utils.analytics import (
    compare_mouse_trajectories,
    find_result_file_by_session_id,
)
from app.utils.document import generate_document_number
from app.utils.sanitizers import sanitize_filename
from app.utils.validators import (
    validate_event_type,
    validate_json_data,
    validate_session_id,
)

from . import api_bp


@api_bp.route("/save_results", methods=["POST"])
@limiter.limit("10 per minute")
@api_key_required
@csrf.exempt
def save_results():
    """
    –ê—Ç–æ–º–∞—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Ç–µ—Å—Ç–∞,
    —Å–æ–∑–¥–∞–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –≤ –ë–î –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.
    –£—Å—Ç–æ–π—á–∏–≤ –∫ race condition –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    try:
        # Pydantic-–≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥—è—â–µ–≥–æ JSON. –≠—Ç–æ—Ç –±–ª–æ–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.
        validated_data = SaveResultsRequest.model_validate(request.get_json())
    except ValidationError as e:
        current_app.logger.warning(
            f"Invalid data from {request.remote_addr}: {e.errors()}"
        )
        return jsonify({"status": "error", "message": "Invalid input data", "details": e.errors()}), 400

    # <--- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –†–∞–±–æ—Ç–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é —Å Pydantic-–æ–±—ä–µ–∫—Ç–æ–º, –∞ –Ω–µ —Å–æ —Å–ª–æ–≤–∞—Ä–µ–º ---
    session_id = validated_data.sessionId
    persistent_id = validated_data.persistentId.get("cookie")
    fp_hash = validated_data.fingerprint.get("privacySafeHash")

    if not persistent_id or not fp_hash:
        return jsonify({"status": "error", "message": "Missing required identifiers"}), 400

    user = None # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º user
    attempt = 1
    max_attempts = 2 # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–≤—É—Ö –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è race condition

    while attempt <= max_attempts:
        try:
        # --- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å (User) ---
            user = User.query.filter_by(persistent_id=persistent_id).with_for_update().first() # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ –≤—Ä–µ–º—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            if not user:
                user_info = validated_data.userInfo
                if not user_info or not user_info.lastName or not user_info.firstName:
                     current_app.logger.warning(f"Missing required userInfo fields for persistent_id {persistent_id}")
                     return jsonify({"status": "error", "message": "Missing required user information (lastName, firstName)"}), 400
                user = User(
                    lastname=user_info.lastName,
                    firstname=user_info.firstName,
                    middlename=user_info.middleName,
                    position=user_info.position,
                    persistent_id=persistent_id,
                )
                db.session.add(user)
                # –ù–ï –î–ï–õ–ê–ï–ú FLUSH –ó–î–ï–°–¨

        # --- "–¶–∏—Ñ—Ä–æ–≤–æ–π –æ—Ç–ø–µ—á–∞—Ç–æ–∫" (Fingerprint) ---
            fingerprint = Fingerprint.query.filter_by(fingerprint_hash=fp_hash).first()
            if not fingerprint:
                fp_data = validated_data.fingerprint.get("privacySafe", {})
                fingerprint = Fingerprint(
                    fingerprint_hash=fp_hash,
                    user_agent=fp_data.get("userAgent"),
                    platform=fp_data.get("platform"),
                    webgl_renderer=fp_data.get("webGLRenderer"),
                )
                db.session.add(fingerprint)
            else:
                fingerprint.last_seen = datetime.now(UTC)

        # --- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (ResultMetadata) - –û–ë–ù–û–í–õ–ï–ù–ò–ï ---
            result = ResultMetadata.query.get(session_id)
            if not result:
                current_app.logger.error(
                    f"Attempted to save results for non-existent session: {session_id}"
                )
                return jsonify({"status": "error", "message": f"Session {session_id} not found"}), 404

        # –†–∞–∑–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Pydantic-–æ–±—ä–µ–∫—Ç–∞
            start_time_dt = validated_data.sessionMetrics.startTime
            end_time_dt = validated_data.sessionMetrics.endTime
            score = validated_data.testResults.percentage # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∏–∂–µ
            start_time_from_dict = validated_data.dict().get("sessionMetrics", {}).get("startTime")
            end_time_from_dict = validated_data.dict().get("sessionMetrics", {}).get("endTime")

        # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç
            if not isinstance(start_time_dt, datetime) and start_time_from_dict:
                current_app.logger.warning(
                    f"Pydantic might have failed to parse startTime: '{start_time_from_dict}' for session {session_id}. Using raw value if available."
                )
                # –ï—Å–ª–∏ start_time_dt –Ω–µ datetime, –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å start_time_from_dict –∑–¥–µ—Å—å,
                # –Ω–æ Pydantic –¥–æ–ª–∂–µ–Ω –±—ã–ª –≤—ã–¥–∞—Ç—å –æ—à–∏–±–∫—É –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ä–∞–Ω—å—à–µ, –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π.
            if not isinstance(end_time_dt, datetime) and end_time_from_dict:
                 current_app.logger.warning(
                    f"Pydantic might have failed to parse endTime: '{end_time_from_dict}' for session {session_id}. Using raw value if available."
                )

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∑–∞–ø–∏—Å–∏
            result.user_id = user.id # –°–≤—è–∑—ã–≤–∞–µ–º —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∏–ª–∏ —Ç–æ–ª—å–∫–æ —á—Ç–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–º user
            result.fingerprint_hash = fp_hash
            result.test_type = validated_data.test_type
            result.score = score # –ò—Å–ø–æ–ª—å–∑—É–µ–º score
            result.start_time = start_time_dt # –ò—Å–ø–æ–ª—å–∑—É–µ–º datetime –∏–∑ Pydantic
            result.end_time = end_time_dt     # –ò—Å–ø–æ–ª—å–∑—É–µ–º datetime –∏–∑ Pydantic
            result.raw_data = validated_data.model_dump(mode="json", by_alias=True)
            result.client_ip = request.remote_addr

        # --- –ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç ---
            document_number = None
            passed = result.score >= current_app.config.get("PASSING_SCORE_THRESHOLD", 80)
            if passed:
                document_number = generate_document_number(db.session)
                result.document_number = document_number

                certificate = Certificate(
                    document_number=document_number,
                    user_fullname=user.full_name, # –ò—Å–ø–æ–ª—å–∑—É–µ–º user
                    user_position=user.position,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º user
                # <--- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–æ–ª–µ test_type –∏ –∑–¥–µ—Å—å ---
                    test_type=validated_data.test_type,
                    score_percentage=result.score,
                    session_id=session_id,
                )
                db.session.add(certificate)

        # --- –§–∏–∫—Å–∞—Ü–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ ---
            db.session.commit() # –í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ–¥–Ω–∏–º –∫–æ–º–º–∏—Ç–æ–º

        # --- –î–µ–π—Å—Ç–≤–∏—è –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---

        # 1. –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            cache.delete_memoized(get_results_api)
            cache.delete_memoized(get_certificates)

        # 2. –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ WebSocket
            socketio.emit("update_needed", {"type": "new_result", "session_id": session_id})

        # <--- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç –º–µ—Ç—Ä–∏–∫ Prometheus ---
            result_status = "passed" if passed else "failed"
            TESTS_COMPLETED_TOTAL.labels(test_type=validated_data.test_type, result=result_status).inc()
            if document_number:
                DOCUMENTS_GENERATED_TOTAL.labels(test_type=validated_data.test_type).inc()
        # -----------------------------------------------

            current_app.logger.info(f"Results saved: session={session_id}, score={result.score}%, doc={document_number}")


            response = {"status": "success", "message": "Results saved successfully", "session_id": session_id}
            if document_number:
                response["officialDocumentNumber"] = document_number

            return jsonify(response), 201 # –£—Å–ø–µ—Ö, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏

        except IntegrityError as e:
            db.session.rollback()
            if "ix_users_persistent_id" in str(e.orig) and attempt < max_attempts:
                 current_app.logger.warning(f"Race condition detected for persistent_id {persistent_id} on attempt {attempt}. Retrying.")
                 attempt += 1 # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
                 continue # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ü–∏–∫–ª–∞
            else:
                 current_app.logger.error(f"DB integrity error (not race condition or retries exceeded): {e}", exc_info=True) # –î–æ–±–∞–≤–ª–µ–Ω–æ exc_info=True
                 return jsonify({"status": "error", "message": "Data conflict"}), 409
        except Exception as e:
            db.session.rollback()
            current_app.logger.error(f"Unexpected error in save_results: {e}", exc_info=True)
            return jsonify({"status": "error", "message": "Internal server error"}), 500
    current_app.logger.error(f"Failed to save results for {session_id} after {max_attempts} attempts due to persistent race condition or other issue.")
    return jsonify({"status": "error", "message": "Failed to save results after multiple attempts"}), 500


@api_bp.route("/log_event", methods=["POST"])
@limiter.limit("60 per minute")
@csrf.exempt
@api_key_required
def log_event():
    data = request.get_json()
    if not data:
        return jsonify({"status": "error", "message": "No data"}), 400

    is_valid, error_msg = validate_json_data(data, ["sessionId", "eventType"])
    if not is_valid:
        return jsonify({"status": "error", "message": error_msg}), 400

    session_id = data["sessionId"]
    event_type = data["eventType"]
    if not validate_session_id(session_id):
        return jsonify({"status": "error", "message": "Invalid session ID"}), 400
    if not validate_event_type(event_type):
        return jsonify({"status": "error", "message": "Invalid event type"}), 400

    try:
        # --- –õ–û–ì–ò–ö–ê –ü–†–û–í–ï–†–ö–ò –ò –°–û–ó–î–ê–ù–ò–Ø –†–û–î–ò–¢–ï–õ–¨–°–ö–û–ô –ó–ê–ü–ò–°–ò ---
        result_metadata = db.session.get(ResultMetadata, session_id)
        if not result_metadata:
            # –ï—Å–ª–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è –∑–∞–ø–∏—Å—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º "—á–µ—Ä–Ω–æ–≤–∏–∫"
            result_metadata = ResultMetadata(session_id=session_id, test_type="pending")
            db.session.add(result_metadata)
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        # –õ–æ–≥–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∞–º–æ–≥–æ —Å–æ–±—ã—Ç–∏—è (proctoring_event)
        details = data.get("details", {})
        details["ip"] = request.remote_addr

        event = ProctoringEvent(
            session_id=session_id,
            event_type=event_type,
            event_timestamp=datetime.fromisoformat(
                data.get("eventTimestamp", datetime.now(UTC).isoformat()).replace(
                    "Z", ""
                )
            ),
            details=details,
            persistent_id=details.get("persistentId"),
            client_ip=request.remote_addr,
            page=details.get("page"),
        )
        db.session.add(event)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ —Å–æ–±—ã—Ç–∏–µ, –∏, –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –æ —Å–µ—Å—Å–∏–∏ –≤ –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        db.session.commit()

        return jsonify({"status": "success"}), 200

    except Exception as e:
        db.session.rollback()
        current_app.logger.error(f"Error in log_event: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Internal error"}), 500


from datetime import datetime, timedelta

from sqlalchemy import and_, case, func, or_


@api_bp.route("/get_behavior_analysis", methods=["GET"])
@admin_required
@cache.memoize(timeout=120)
def get_behavior_analysis():
    """
    Optimized behavioral analysis using database aggregation.
    All heavy lifting done by PostgreSQL.
    """
    try:
        # Configuration
        config = current_app.config
        thresholds = config.get(
            "BEHAVIOR_THRESHOLDS",
            {"min_score": 90, "max_test_duration_sec": 180, "min_engagement_score": 15},
        )

        # Build optimized query using SQLAlchemy ORM
        # Subquery for study sessions with engagement metrics
        study_subquery = (
            db.session.query(
                ProctoringEvent.session_id.label("study_session_id"),
                ProctoringEvent.persistent_id,
                ProctoringEvent.client_ip,
                func.min(ProctoringEvent.event_timestamp).label("study_start"),
                func.max(ProctoringEvent.event_timestamp).label("study_end"),
                func.sum(
                    case(
                        (
                            ProctoringEvent.event_type == "module_view_time",
                            func.cast(
                                ProctoringEvent.details["duration"].as_string(),
                                db.Integer,
                            ),
                        ),
                        else_=0,
                    )
                ).label("total_view_time"),
                func.max(
                    case(
                        (
                            ProctoringEvent.event_type == "scroll_depth_milestone",
                            func.cast(
                                func.replace(
                                    ProctoringEvent.details["depth"].as_string(),
                                    "%",
                                    "",
                                ),
                                db.Integer,
                            ),
                        ),
                        else_=0,
                    )
                ).label("max_scroll_depth"),
                func.sum(
                    case(
                        (ProctoringEvent.event_type == "self_check_answered", 1),
                        else_=0,
                    )
                ).label("self_check_count"),
            )
            .filter(
                ProctoringEvent.event_type.in_(
                    [
                        "study_started",
                        "module_view_time",
                        "scroll_depth_milestone",
                        "self_check_answered",
                    ]
                )
            )
            .group_by(
                ProctoringEvent.session_id,
                ProctoringEvent.persistent_id,
                ProctoringEvent.client_ip,
            )
            .subquery()
        )

        # Main query joining test results with study metrics
        suspicious_query = (
            db.session.query(
                ResultMetadata.session_id,
                ResultMetadata.score,
                ResultMetadata.test_type,
                ResultMetadata.client_ip,
                User.lastname,
                User.firstname,
                User.middlename,
                User.position,
                func.extract(
                    "epoch", ResultMetadata.end_time - ResultMetadata.start_time
                ).label("test_duration"),
                study_subquery.c.total_view_time,
                study_subquery.c.max_scroll_depth,
                study_subquery.c.self_check_count,
                func.extract(
                    "epoch", study_subquery.c.study_end - study_subquery.c.study_start
                ).label("study_duration"),
            )
            .join(User, ResultMetadata.user_id == User.id)
            .outerjoin(
                study_subquery,
                and_(
                    or_(
                        User.persistent_id == study_subquery.c.persistent_id,
                        ResultMetadata.client_ip == study_subquery.c.client_ip,
                    ),
                    study_subquery.c.study_start < ResultMetadata.start_time,
                    study_subquery.c.study_start
                    > ResultMetadata.start_time - timedelta(hours=24),
                ),
            )
            .filter(
                ResultMetadata.score >= thresholds["min_score"],
                func.extract(
                    "epoch", ResultMetadata.end_time - ResultMetadata.start_time
                )
                < thresholds["max_test_duration_sec"],
            )
        )

        # Execute query and process results
        suspicious_sessions = []
        for row in suspicious_query.all():
            # Calculate engagement score
            engagement_score = 0
            if row.total_view_time:
                engagement_score += int(row.total_view_time / 60)
            if row.max_scroll_depth:
                if row.max_scroll_depth >= 95:
                    engagement_score += 10
                elif row.max_scroll_depth >= 50:
                    engagement_score += 5
            if row.self_check_count:
                engagement_score += row.self_check_count * 2

            # Check if suspicious
            if engagement_score < thresholds["min_engagement_score"]:
                suspicious_sessions.append(
                    {
                        "sessionId": row.session_id,
                        "userInfo": {
                            "lastName": row.lastname,
                            "firstName": row.firstname,
                            "middleName": row.middlename,
                            "position": row.position,
                        },
                        "testResult": {
                            "score": row.score,
                            "duration": (
                                int(row.test_duration) if row.test_duration else 0
                            ),
                        },
                        "studyInfo": {
                            "duration": (
                                int(row.study_duration) if row.study_duration else 0
                            ),
                            "engagementScore": engagement_score,
                        },
                        "reason": f"High score ({row.score}%) with fast completion "
                        f"({int(row.test_duration)}s) and low study engagement "
                        f"(Score: {engagement_score})",
                    }
                )

        current_app.logger.info(
            f"Behavior analysis found {len(suspicious_sessions)} suspicious sessions"
        )
        return jsonify(suspicious_sessions), 200

    except Exception as e:
        current_app.logger.error(f"Error in behavior analysis: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Analysis failed"}), 500


from app.utils.analytics import find_result_file_by_session_id


@api_bp.route("/get_full_result/<session_id>", methods=["GET"])
@admin_required
def get_full_result_data(session_id: str):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–µ JSON-–¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
    """
    # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å —Å–∞–º–∞ –¥–µ–ª–∞–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é –∏ –∏—â–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –ë–î
    result_data = find_result_file_by_session_id(session_id)

    if result_data:
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–∞–π–¥–µ–Ω—ã, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö
        return jsonify(result_data), 200
    else:
        # –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ None (ID –Ω–µ–≤–∞–ª–∏–¥–µ–Ω –∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω), –æ—Ç–¥–∞–µ–º 404
        return jsonify({"status": "error", "message": "Result not found"}), 404


from app.utils.analytics import (
    compare_mouse_trajectories,
    find_result_file_by_session_id,
)


@api_bp.route("/analyze_mouse", methods=["POST"])
@admin_required  # <--- –î–û–ë–ê–í–¨–¢–ï –≠–¢–û–¢ –î–ï–ö–û–†–ê–¢–û–†
@limiter.limit("5 per minute")
def analyze_mouse():
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –º—ã—à–∏, –ø–æ–ª—É—á–∞—è –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –ë–î,
    –∏ –∏–∑–º–µ—Ä—è–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
    """
    try:
        data = request.get_json()
        session_ids = data.get("session_ids")

        if not session_ids or len(session_ids) < 2:
            return jsonify({"error": "–ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —Å–µ—Å—Å–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"}), 400

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –ë–î
        trajectories = {}
        for sid in session_ids:
            result_data = find_result_file_by_session_id(sid)
            if not result_data:
                current_app.logger.warning(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–µ—Å—Å–∏–∏ {sid} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ë–î")
                continue

            per_question = result_data.get("behavioralMetrics", {}).get(
                "perQuestion", []
            )
            trajectories[sid] = {}
            for i, q_data in enumerate(per_question):
                movements = q_data.get("mouseMovements")
                if movements:
                    trajectories[sid][i] = movements

        # –õ–æ–≥–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
        results = {}
        sid_list = list(session_ids)

        for i in range(len(sid_list)):
            for j in range(i + 1, len(sid_list)):
                s1, s2 = sid_list[i], sid_list[j]
                pair_key = f"{s1}_vs_{s2}"
                results[pair_key] = {}

                common_qs = set(trajectories.get(s1, {}).keys()) & set(
                    trajectories.get(s2, {}).keys()
                )

                for q_idx in common_qs:
                    t1 = trajectories[s1][q_idx]
                    t2 = trajectories[s2][q_idx]

                    # <--- –ù–ê–ß–ê–õ–û –ò–ó–ú–ï–ù–ï–ù–ò–Ø: –ò–ó–ú–ï–†–ï–ù–ò–ï –î–õ–ò–¢–ï–õ–¨–ù–û–°–¢–ò --->
                    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–µ–º–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
                    with ANALYSIS_DURATION_SECONDS.time():
                        similarity = compare_mouse_trajectories(t1, t2)
                    # –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–ª–æ–∫–∞ `with` –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç
                    # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø–∏—Å–∞–Ω–∞ –≤ –º–µ—Ç—Ä–∏–∫—É Prometheus.
                    # <--- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø --->

                    results[pair_key][q_idx] = round(similarity, 1)

        return jsonify(results), 200

    except Exception as e:
        current_app.logger.error(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ –¥–≤–∏–∂–µ–Ω–∏–π –º—ã—à–∏: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Internal server error"}), 500


@api_bp.route("/get_results", methods=["GET"])
@admin_required
@cache.cached(timeout=60, query_string=True)
def get_results_api():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¢–û–õ–¨–ö–û –ó–ê–í–ï–†–®–ï–ù–ù–´–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏–∑ –ë–î.
    –§–∏–ª—å—Ç—Ä—É–µ—Ç –∑–∞–ø–∏—Å–∏ 'pending' –∏ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç score –∏–ª–∏ end_time.
    """
    try:
        page = request.args.get("page", 1, type=int)
        per_page = request.args.get("per_page", 20, type=int)
        max_per_page = current_app.config.get("MAX_RESULTS_PER_PAGE", 1000)

        if not (1 <= page <= 1000 and 1 <= per_page <= max_per_page):
            return (
                jsonify(
                    {"status": "error", "message": "Invalid pagination parameters"}
                ),
                400,
            )

        # –û—Å–Ω–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞: –ø–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Å—Ä–∞–∑—É –ø–æ–¥–≥—Ä—É–∂–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è,
        base_query = ResultMetadata.query.options(
            db.joinedload(ResultMetadata.user)
        )
        # --- üëá –î–û–ë–ê–í–õ–ï–ù–´ –§–ò–õ–¨–¢–†–´ –î–õ–Ø –ó–ê–í–ï–†–®–ï–ù–ù–´–• –¢–ï–°–¢–û–í üëá ---
        base_query = base_query.filter(
            ResultMetadata.score.isnot(None),    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –±–∞–ª–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω
            ResultMetadata.end_time.isnot(None), # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω
            ResultMetadata.user_id.isnot(None)   # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏–≤—è–∑–∞–Ω
        ).order_by(ResultMetadata.start_time.desc())

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –ø–∞–≥–∏–Ω–∞—Ü–∏—é SQLAlchemy - —ç—Ç–æ –ø—Ä–æ—â–µ –∏ –Ω–∞–¥–µ–∂–Ω–µ–µ
        pagination = base_query.paginate(page=page, per_page=per_page, error_out=False)
        results_from_db = pagination.items
        total = pagination.total

        results = []
        for row in results_from_db:
            # score —Ç–µ–ø–µ—Ä—å —Ç–æ—á–Ω–æ –Ω–µ None –∏–∑-–∑–∞ —Ñ–∏–ª—å—Ç—Ä–∞ .isnot(None)
            score = row.score
            if score >= 90:
                grade_class, grade_text = "excellent", "–û—Ç–ª–∏—á–Ω–æ"
            elif score >= 80:
                grade_class, grade_text = "good", "–•–æ—Ä–æ—à–æ"
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º >= 70 –¥–ª—è –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ, –∫–∞–∫ –≤ 117-test.html
            elif score >= 70:
                grade_class, grade_text = "satisfactory", "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º >= 60 –¥–ª—è –ù–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ, –∫–∞–∫ –≤ 117-test.html
            elif score >= 60:
                 grade_class, grade_text = "unsatisfactory", "–ù–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"
            else:
                grade_class, grade_text = "poor", "–ü–ª–æ—Ö–æ" # –î–ª—è < 60

            results.append(
                {
                    "sessionId": row.session_id,
                    "testType": row.test_type,
                    "clientIp": row.client_ip,
                    "userInfo": {
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º row.user –Ω–∞ —Å–ª—É—á–∞–π —Ä–µ–¥–∫–∏—Ö –æ—à–∏–±–æ–∫ —Å–≤—è–∑–∏, —Ö–æ—Ç—è joinload –¥–æ–ª–∂–µ–Ω –ø–æ–º–æ—á—å
                        "lastName": row.user.lastname if row.user else "N/A",
                        "firstName": row.user.firstname if row.user else "N/A",
                    },
                    "testResults": {
                        "percentage": score,
                        "grade": {"class": grade_class, "text": grade_text},
                    },
                    "sessionMetrics": {
                        # start_time –∏ end_time —Ç–µ–ø–µ—Ä—å —Ç–æ—á–Ω–æ –Ω–µ None –∏–∑-–∑–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                        "startTime": row.start_time.isoformat(),
                        "endTime": row.end_time.isoformat(),
                    },
                }
            )

        return jsonify(
            {"results": results, "page": page, "per_page": per_page, "total": total}
        )

    except Exception as e:
        current_app.logger.error(f"Error in get_results_api: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Error retrieving results"}), 500


@api_bp.route("/get_certificates", methods=["GET"])
@admin_required
@cache.memoize(timeout=360)
def get_certificates():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–µ—Å—Ç—Ä –≤—Å–µ—Ö –≤—ã–¥–∞–Ω–Ω—ã—Ö —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤ –∏–∑ –ë–î."""
    try:
        # –ó–∞–ø—Ä–æ—Å –∫ –ë–î —á–µ—Ä–µ–∑ SQLAlchemy
        certs_from_db = Certificate.query.order_by(Certificate.issue_date.desc()).all()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–∏ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞ to_dict()
        certificates = [cert.to_dict() for cert in certs_from_db]
        return jsonify(certificates), 200

    except Exception as e:
        current_app.logger.error(f"Error in get_certificates: {e}", exc_info=True)
        return jsonify({"status": "error", "message": "Internal server error"}), 500


@api_bp.route("/get_events/<session_id>", methods=["GET"])
@admin_required
def get_events(session_id: str):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —Å–æ–±—ã—Ç–∏—è –ø—Ä–æ–∫—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å–µ—Å—Å–∏–∏."""
    try:
        if not validate_session_id(session_id):
            return jsonify({"status": "error", "message": "Invalid session ID"}), 400

        events_from_db = (
            ProctoringEvent.query.filter_by(session_id=session_id)
            .order_by(ProctoringEvent.event_timestamp.asc())
            .all()
        )
        events = [event.to_dict() for event in events_from_db]

        return jsonify(events), 200
    except Exception as e:
        current_app.logger.error(
            f"Error in get_events for session {session_id}: {e}", exc_info=True
        )
        return jsonify({"status": "error", "message": "Internal server error"}), 500


@api_bp.route("/get_abandoned_sessions", methods=["GET"])
@admin_required
@cache.memoize(timeout=60)
def get_abandoned_sessions():
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã–µ —Å–µ—Å—Å–∏–∏...
    –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–µ–ø–µ—Ä—å –ø—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å userInfo –∏–∑ –¥–µ—Ç–∞–ª–µ–π –ø–µ—Ä–≤–æ–≥–æ —Å–æ–±—ã—Ç–∏—è,
    –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ.
    """
    try:
        passing_score = current_app.config.get('PASSING_SCORE_THRESHOLD', 80)
        # CTE –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π
        successful_sessions = db.session.query(
            ResultMetadata.session_id
        ).filter(
            ResultMetadata.session_id.isnot(None),
            ResultMetadata.score >= passing_score # <-- –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é passing_score
        ).cte('successful_sessions')

        first_event = db.session.query(
            ProctoringEvent.session_id,
            ProctoringEvent.event_type,
            ProctoringEvent.event_timestamp,
            ProctoringEvent.details,
            ProctoringEvent.persistent_id,
            func.row_number().over(
                partition_by=ProctoringEvent.session_id,
                order_by=ProctoringEvent.event_timestamp.asc()
            ).label('rn')
        ).filter(
            ProctoringEvent.event_type.in_(['test_started', 'study_started']), # –î–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–ø—è—Ç–∞—è
            ProctoringEvent.details.isnot(None)
        ).cte('first_event')

        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—Ä–æ—à–µ–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π
        abandoned_query = db.session.query(
            ProctoringEvent.session_id,
            func.max(first_event.c.event_type).label('session_type'),
            func.min(ProctoringEvent.event_timestamp).label('start_time'),
            func.max(cast(first_event.c.details, Text)).label('first_event_details_text'),
            func.max(first_event.c.persistent_id).label('persistent_id'),
             # ... (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏) ...
            func.count(case((ProctoringEvent.event_type == 'focus_loss', 1))).label('focus_loss_count'),
            func.count(case((ProctoringEvent.event_type == 'screenshot_attempt', 1))).label('screenshot_count'),
            func.count(case((ProctoringEvent.event_type == 'print_attempt', 1))).label('print_count')
        ).select_from(
            ProctoringEvent
        ).outerjoin(
            first_event,
            and_(
                ProctoringEvent.session_id == first_event.c.session_id,
                first_event.c.rn == 1
            )
        ).filter(
            ~ProctoringEvent.session_id.in_(
                db.session.query(successful_sessions.c.session_id)
            )
        ).group_by(
            ProctoringEvent.session_id
        ).order_by(
            func.min(ProctoringEvent.event_timestamp).desc()
        )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        abandoned_sessions = abandoned_query.all()

        if not abandoned_sessions:
            current_app.logger.info("No abandoned/unsuccessful sessions found.")
            return jsonify([]), 200

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ persistent_id –¥–ª—è batch-–∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        persistent_ids = {s.persistent_id for s in abandoned_sessions if s.persistent_id}

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
        users_map = {}
        if persistent_ids:
            users = User.query.filter(User.persistent_id.in_(persistent_ids)).all()
            users_map = {user.persistent_id: user for user in users}

        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ —Å–µ—Å—Å–∏–π
        session_type_map = {
            'test_started': 'test',
            'study_started': 'study'
        }

        results = []
        for session in abandoned_sessions:
            first_event_details = {} # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∫ –ø—É—Å—Ç–æ–π dict
            client_ip = "N/A"
            user_info_from_event = {} # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∫ –ø—É—Å—Ç–æ–π dict
            if session.first_event_details_text:
                try:
                    first_event_details = json.loads(session.first_event_details_text)
                    if isinstance(first_event_details, dict):
                         client_ip = first_event_details.get('ip', 'N/A')
                         user_info_from_event = first_event_details.get('userInfo', {})
                    else:
                         current_app.logger.warning(
                             f"Parsed details is not a dict for session {session.session_id}"
                         )
                         first_event_details = {} # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–æ –ø—É—Å—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
                         user_info_from_event = {}
                except (json.JSONDecodeError, TypeError) as e:
                    current_app.logger.warning(
                        f"Could not parse details text for session {session.session_id}: {e}"
                    )
                    first_event_details = {} # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–æ –ø—É—Å—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
                    user_info_from_event = {}
            # --- –õ–û–ì–ò–ö–ê –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø USER INFO ---
            user = users_map.get(session.persistent_id)

            if user:
                # 1. –õ—É—á—à–∏–π —Å–ª—É—á–∞–π: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞–π–¥–µ–Ω –≤ —Ç–∞–±–ª–∏—Ü–µ users
                user_info = {'lastName': user.lastname, 'firstName': user.firstname}
            elif user_info_from_event and user_info_from_event.get('lastName'):
                # 2. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å userInfo –≤ –¥–µ—Ç–∞–ª—è—Ö —Å–æ–±—ã—Ç–∏—è
                user_info = {
                    'lastName': user_info_from_event.get('lastName', 'N/A'),
                    'firstName': user_info_from_event.get('firstName', 'N/A'),
                    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Ç—á–µ—Å—Ç–≤–æ –∏ –¥–æ–ª–∂–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    # 'middleName': user_info_from_event.get('middleName'),
                    'source': 'event_log'
                }
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–º–µ—Ç–∫—É, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–≥–∞
            elif session.persistent_id:
                # 3. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ—Ç, userInfo –≤ —Å–æ–±—ã—Ç–∏–∏ –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å persistent_id
                user_info = {'lastName': 'N/A', 'firstName': f"ID: {str(session.persistent_id)[:8]}..."}
            else:
                # 4. –°–æ–≤—Å–µ–º –Ω–∏—á–µ–≥–æ –Ω–µ—Ç
                user_info = {'lastName': 'N/A', 'firstName': 'N/A'}
            # --- –ö–û–ù–ï–¶ –õ–û–ì–ò–ö–ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø USER INFO ---

            results.append({
                'sessionId': session.session_id,
                'sessionType': session_type_map.get(session.session_type, 'unknown'),
                'startTime': session.start_time.isoformat() if session.start_time else 'N/A',
                'userInfo': user_info, # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–π userInfo
                'clientIp': client_ip,
                'violationCounts': {
                    'focusLoss': session.focus_loss_count or 0,
                    'screenshots': session.screenshot_count or 0,
                    'prints': session.print_count or 0
                }
            })

        current_app.logger.info(
            f"Returning {len(results)} abandoned/unsuccessful sessions (safe JSON parsing)"
        )
        return jsonify(results), 200

    except DBAPIError as e: # –õ–æ–≤–∏–º –æ—à–∏–±–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–≤–∫–ª—é—á–∞—è –æ—à–∏–±–∫–∏ SQL —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞/—Ç–∏–ø–æ–≤)
         db.session.rollback()
         current_app.logger.error(f"Database error in get_abandoned_sessions: {e}", exc_info=True)
         return jsonify({'status': 'error', 'message': 'Database error during analysis'}), 500
    except Exception as e:
        db.session.rollback() # –û—Ç–∫–∞—Ç –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        current_app.logger.error(
            f"Unexpected error in get_abandoned_sessions: {e}",
            exc_info=True
        )
        return jsonify({'status': 'error','message': 'Internal server error'}), 500
